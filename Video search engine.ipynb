{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d3156af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b0d8b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr\n",
    "import moviepy.editor as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1eb8118b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104.50999999999999\n",
      "MoviePy - Writing audio in converted1.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Please wait ...\n",
      "Speech to text conversion successfull.\n"
     ]
    }
   ],
   "source": [
    "import speech_recognition as sr\n",
    "import moviepy.editor as me\n",
    "\n",
    "VIDEO_FILE = \"video2.mp4\"\n",
    "OUTPUT_AUDIO_FILE = \"converted1.wav\"\n",
    "OUTPUT_TEXT_FILE = \"recognized1.txt\"\n",
    "try:\n",
    "    video_clip = me.VideoFileClip(r\"{}\".format(VIDEO_FILE))\n",
    "    print(video_clip.duration)\n",
    "    video_clip.audio.write_audiofile(r\"{}\".format(OUTPUT_AUDIO_FILE))\n",
    "    recognizer =  sr.Recognizer()\n",
    "    audio_clip = sr.AudioFile(\"{}\".format(OUTPUT_AUDIO_FILE))\n",
    "    with audio_clip as source:\n",
    "        audio_file = recognizer.record(source)\n",
    "    print(\"Please wait ...\")\n",
    "    result = recognizer.recognize_google(audio_file)\n",
    "    with open(OUTPUT_TEXT_FILE, 'w') as file:\n",
    "        file.write(result)\n",
    "        print(\"Speech to text conversion successfull.\")\n",
    "except Exception as e:\n",
    "    print(\"Attempt failed -- \", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a5ba23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from string import punctuation\n",
    "from heapq import nlargest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d23c7c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"\"\n",
    "with open('recognized1.txt' , 'r') as f:\n",
    "    s = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9179cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(text, per):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    doc= nlp(text)\n",
    "    tokens=[token.text for token in doc]\n",
    "    word_frequencies={}\n",
    "    for word in doc:\n",
    "        if word.text.lower() not in list(STOP_WORDS):\n",
    "            if word.text.lower() not in punctuation:\n",
    "                if word.text not in word_frequencies.keys():\n",
    "                    word_frequencies[word.text] = 1\n",
    "                else:\n",
    "                    word_frequencies[word.text] += 1\n",
    "    max_frequency=max(word_frequencies.values())\n",
    "    for word in word_frequencies.keys():\n",
    "        word_frequencies[word]=word_frequencies[word]/max_frequency\n",
    "    sentence_tokens= [sent for sent in doc.sents]\n",
    "    sentence_scores = {}\n",
    "    for sent in sentence_tokens:\n",
    "        for word in sent:\n",
    "            if word.text.lower() in word_frequencies.keys():\n",
    "                if sent not in sentence_scores:                            \n",
    "                    sentence_scores[sent]=word_frequencies[word.text.lower()]\n",
    "                else:\n",
    "                    sentence_scores[sent]+=word_frequencies[word.text.lower()]\n",
    "    select_length=int(len(sentence_tokens)*per)\n",
    "    summary=nlargest(select_length, sentence_scores,key=sentence_scores.get)\n",
    "    final_summary=[word.text for word in summary]\n",
    "    summary=''.join(final_summary)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c62d2d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = summarize(s , 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3ccfbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b0222c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy  \n",
    "import re\n",
    "import json\n",
    "import pymongo \n",
    "import datetime\n",
    "import sys\n",
    "import xlrd\n",
    "import xlsxwriter\n",
    "import openpyxl\n",
    "from openpyxl import Workbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e44452",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# You can install a downloaded python whl file via the python package manager pip e.g.:\n",
    "\n",
    "# pip install en_core_web_sm-3.1.0-py3-none-any.whl\n",
    "\n",
    "# You can download a en_core_web_sm from this page:\n",
    "\n",
    "# https://github.com/explosion/spacy-models/releases/tag/en_core_web_sm-3.1.0\n",
    "\n",
    "# The example from the spacy front page looks easy, have you tried the commented first lines in your terminal? Example from spacy page:\n",
    "\n",
    "# pip install -U spacy\n",
    "# python -m spacy download en_core_web_sm\n",
    "import spacy\n",
    "\n",
    "# Load English tokenizer, tagger, parser and NER\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a5cd645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good morning chetan singh i saharanpur uttar pradesh currently residing meerut uttar pradesh currently engineering undergraduate student pursuing bachelors technology computer science engineering lovely professional university phagwara punjab i.get schooling meerut uttar pradesh kendriya vidyalaya meerut i basically come farming oriented family family sister talk grandfather person farming measure odisha coming loop father mr vijendra kumar currently serving indian army army medical corps currently posted hyderabad mother mrs renu housewife i younger sister currently studying 11th standard talking hobbies include reading web novels watching\n"
     ]
    }
   ],
   "source": [
    "ignore = list(STOP_WORDS)\n",
    "s = summary.split()\n",
    "l = []\n",
    "for i in s:\n",
    "    if i not in ignore:\n",
    "        l.append(i.lower())\n",
    "set(l)\n",
    "l = \" \".join(l)\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0a98791",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lovely\n",
      "student\n",
      "mrs\n",
      "reading\n",
      "undergraduate\n",
      "father\n",
      "uttar\n",
      "loop\n",
      "indian\n",
      "singh\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "from string import punctuation\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "def get_hotwords(text):\n",
    "    result = []\n",
    "    pos_tag = ['PROPN', 'ADJ', 'NOUN'] \n",
    "    doc = nlp(text.lower()) \n",
    "    for token in doc:\n",
    "        if(token.text in nlp.Defaults.stop_words or token.text in punctuation):\n",
    "            continue\n",
    "        if(token.pos_ in pos_tag):\n",
    "            result.append(token.text)\n",
    "    return result\n",
    "new_text = summary\n",
    "output = set(get_hotwords(new_text))\n",
    "most_common_list = Counter(output).most_common(10)\n",
    "for item in most_common_list:\n",
    "    print(item[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17880cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download(\"words\")\n",
    "# from nltk.corpus import words\n",
    "\n",
    "# set(words.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afaa9bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download()\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "# from nltk.corpus import stopwords\n",
    "\n",
    "# set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d59c403",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bigrams(string):\n",
    "    \"\"\"\n",
    "    Take a string and return a list of bigrams.\n",
    "    \"\"\"\n",
    "    if string is None:\n",
    "        return \"\"\n",
    "\n",
    "    s = string.lower()\n",
    "    return [s[i : i + 2] for i in list(range(len(s) - 1))]\n",
    "\n",
    "def simon_similarity(str1, str2):\n",
    "    \"\"\"\n",
    "    Perform bigram comparison between two strings\n",
    "    and return a percentage match in decimal form.\n",
    "    \"\"\"\n",
    "    pairs1 = get_bigrams(str1)\n",
    "    pairs2 = get_bigrams(str2)\n",
    "    union = len(pairs1) + len(pairs2)\n",
    "\n",
    "    if union == 0 or union is None:\n",
    "        return 0\n",
    "\n",
    "    hit_count = 0\n",
    "    for x in pairs1:\n",
    "        for y in pairs2:\n",
    "            if x == y:\n",
    "                hit_count += 1\n",
    "                break\n",
    "    return (2.0 * hit_count) / union"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
